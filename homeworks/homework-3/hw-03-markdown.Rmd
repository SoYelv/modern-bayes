---
title: 'Homework 3'
author: "STA-360-602"
output: pdf_document
indent: true
documentclass: article
---

**There is no reproducibility component to this homework, so you only need to upload this assignment to Gradescope. You do not need to submit your solution to the lab exercise since it's not worth any points.**\

1. *Lab component* (0 points total) Please refer to module 2 and lab 3 and complete tasks 3---5. **This will not be graded as the entire solution is already posted. You will still be responsible for this material on the exam.**

  (a) (0) Task 3
  (b) (0) Task 4
  (c) (0) Task 5
  
\newpage
  
Total points: Q1 (0) + Q2 (5) + Q3 (5)  = 10 points total
  
2. (5 points total) *The Uniform-Pareto*\
**The goal of this problem is to continue getting more practice calculating the posterior distribution.**\

Suppose $a < x < b.$ Consider the notation $I_{(a,b)}(x),$ where $I$ denotes the indicator function. We define $I_{(a,b)}(x)$ to be the following:

$$
I_{(a,b)}(x)=
\begin{cases} 
1 & \text{if $a < x < b$,}
\\
0 &\text{otherwise.}
\end{cases}
$$

Let X be a random variable and let x be an observed value. Let 

$$
\begin{aligned}
X=x \mid \theta &\sim \text{Uniform}(0,\theta)\\
\theta &\sim \text{Pareto}(\alpha,\beta),
\end{aligned}
$$
where $p(\theta) = \dfrac{\alpha\beta^\alpha}{\theta^{\alpha+1}}I_{(\beta,\infty)}(\theta).$ Write out the likelihood $p(X=x\mid \theta).$ 

Then calculate the posterior distribution of $\theta|X=x.$  

\textbf{Hint 1}: To set up this problem, do the following:

$$p(\theta \mid x) \propto \frac{1}{\theta} I_{(0,\theta)}(x)
\times 
\frac{\alpha \beta^{\alpha}}
{\theta^{\alpha + 1}}
I_{(\beta, \infty)}(\theta).$$

\textbf{Hint 2}: You cannot drop the indicators. You must write the indicators as functions of $\theta$ since this is the random variable. 

\textbf{Hint 3}: It will end up being an updated Pareto. 
  
3. (5  points total) *The Bayes estimator or Bayes rule*\
**The goal of this problem is to practice a similar problem that we considered in Module 2, where we derived the Bayes rule under squared error loss and found the result was the posterior mean.**

(a) (2 pts) Find the Bayes estimator (or Bayes rule) when the loss function is  $L(\theta, \delta(x))~=~c~(\theta-\delta(x))^2,$ where $\textcolor{red}{c >0}$ is a constant. 

(b) (3 pts) Derive the Bayes estimator (or Bayes rule) when $L(\theta, \delta(x)) = w(\theta) (g(\theta)-\delta(x))^2.$ Assume that $w(\theta) > 0.$

Hint: Do so without writing any integrals. Note that you can write $\rho(\theta,\delta(x)) =  E[L(\theta,\delta(x))|X].$  Don't forget to prove or state why the Bayes rule(s) are unique.